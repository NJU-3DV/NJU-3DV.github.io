<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="High-Fidelity Free-view synthesis of emotional 3D talking head.">
  <meta name="keywords" content="3DGS, Talking Head, Multi-View Synthesis (MVS)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head</title>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="http://zhuhao.cc/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More our research
          </a>
          <!-- 请把剩下几个工作都链接都放这里头 -->
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://nju-3dv.github.io/projects/Head360/">
              Head360 (ECCV2024)
            </a>
            <a class="navbar-item" href="https://mhwu2017.github.io/">
              Describe3D (CVPR2023) 
            </a>            <a class="navbar-item" href="https://longwg.github.io/projects/RAFaRe/">
              RAFaRe (AAAI 2023) 
            </a>            <a class="navbar-item" href="https://yiyuzhuang.github.io/mofanerf/">
              MoFaNeRF (ECCV2022) 
            </a>            <a class="navbar-item" href="https://humanaigc.github.io/vivid-talk/">
              VividTalk
            </a>            <a class="navbar-item" href="https://jixinya.github.io/projects/EAMM/">
              EAMM (SIGGRAPH Conf. 2022) 
            </a>            <a class="navbar-item" href="https://yuanxunlu.github.io/projects/LiveSpeechPortraits/">
              LSP (SIGGRAPH Asia 2021) 
            </a>            <a class="navbar-item" href="https://jixinya.github.io/projects/evp/">
              EVP (CVPR 2021) 
            </a>            <a class="navbar-item" href="https://github.com/zhuhao-nju/facescape/">
              FaceScape
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <!-- 这里整个section都要改 -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span class="emotalk">EmoTalk3D</span>: High-Fidelity Free-View
              Synthesis of Emotional 3D Talking Head</h1>

            <div class="is-size-6 publication-authors">
              <!-- 没有链接的名称样式 -->
              <span class="author-block">
                <a href="">Qianyun He</a><sup>1</sup>,</span>
              </span>
              <!-- 有链接的名称样式 -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=sy_WtmcAAAAJ">Xinya Ji</a><sup>1</sup>,</span>
              </span>
              <span class="author-block">
                <a href="">Yicheng Gong</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ZBozF3sAAAAJ">Yuanxun Lu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lXidlSUAAAAJ">Zhengyu Diao</a><sup>1</sup>,</span>
              </span>
              <span class="author-block">
                <a href="">Linjia Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yoyo000.github.io/">Yao Yao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/zhusiyucs/home">Siyu Zhu</a><sup>2</sup>,</span>
              </span>
              <span class="author-block">
                <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <!-- <a href="https://ieeexplore.ieee.org/author/37089011822">Songcen Xu</a><sup>4</sup>, -->
                <a href="">Songcen Xu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <!-- <a href="https://scholar.google.com/citations?hl=zh-CN&user=CseafDAAAAAJ">Xiaofei Wu</a><sup>4</sup>, -->
                <a href="">Xiaofei Wu</a><sup>3</sup>,
              </span> <span class="author-block">
                <a href="">Zixiao Zhang</a><sup>3</sup>,</span>
              </span>
              <span class="author-block">
                <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://zhuhao.cc/home/">Hao Zhu</a><sup>1&#x2709;</sup>
              </span>
            </div><br>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup> State Key Laboratory for Novel Software Technology, Nanjing University, China,</span><br>
              <span class="author-block"><sup>2</sup> Fudan University, Shanghai, China &nbsp;&nbsp;</span>
              <span class="author-block"><sup>3</sup> Huawei Noah's Ark Lab</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. 请上传完arxiv后更新这两条 -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Coming soon)</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (Coming soon)</span>
                  </a>
                </span> -->

                <!-- Video Link. -->
                <span class="link-block">
                  <a href="#our_video"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="#Data_Request" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data Request</span>
                  </a>
                  <!-- <div class="content is-5">
                    <sup>* Due to privacy and copyright issues, please fill in the
                      <a href="./static/license/License_Agreement_EmoTalk3D.docx" download>License Agreement</a>, then click this button to send us an email with a
                      non-commercial use request.</sup>

                  </div> -->
              </div>
              <img src="./static/images/fig_identity.svg" width="960" alt="">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite significant progress in the field of 3D talking heads, prior methods still suffer from
              multi-view consistency and a lack of emotional expressiveness. To address these issues, we collect
              <span class="emotalk">EmoTalk3D</span> dataset with calibrated multi-view videos, emotional
              annotations, and per-frame 3D geometry. Besides, We present a novel approach for synthesizing
              emotion-controllable, featuring enhanced lip synchronization and rendering quality.
            </p>
            <p>
              By training on the <span class="emotalk">EmoTalk3D</span> dataset, we propose a
              <i>"Speech-to-Geometry-to-Appearance"</i>
              mapping framework that first predicts faithful 3D geometry sequence from the audio features, then
              the appearance of a 3D talking head represented by 4D Gaussians is synthesized from the predicted
              geometry. The appearance is further disentangled into canonical and dynamic Gaussians, learned
              from multi-view videos, and fused to render free-view talking head animation.
            </p>
            <p>
              Moreover, our model extracts emotion labels from the input speech and enables controllable emotion
              in the generated talking heads. Our method exhibits improved rendering quality and stability in
              lip motion generation while capturing dynamic facial details such as wrinkles and subtle
              expressions.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Method</h2>
          <img src="./static/images/Method.png" width="1080" alt="">
          <div class="content has-text-justified">
            <p>
              <strong>Overall Pipeline.</strong>The pipeline consists of five modules:
              1) Emotion-content;
              Disentangle Encoder that parses content features and emotion features from input speech;
              2) Speech-to-Geometry Network (S2GNet) that predicts dynamic 3D pointclouds from the features;
              3) Gaussian Optimization and Completion Module for establishing a canonical appearance;
              4) Geometry-to-Appearance Network (G2ANet) that synthesizes facial appearance based on dynamic 3D point
              cloud;
              and 5) Rendering module for rendering dynamic Gaussians into free-view animations.
            </p>
          </div>
        </div>
      </div>
      <!-- Method. -->

      <!-- Dataset. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Dataset</h2>
          <video id="dataset_video" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dataset_cut.mp4" type="video/mp4">
          </video>
          <div class="content has-text-justified">
            <p>
              We establish <span class="emotalk">EmoTalk3D</span> dataset, an emotion-annotated multi-view talking head dataset with per-frame 3D
              facial shapes.
              <span class="emotalk">EmoTalk3D</span> dataset provides audio, per-frame multi-view images, camera paramters and corresponding
              reconstructed 3D shapes.
              The data have been released to public for non-commercial research purpose.
            </p>
          </div>
        </div>
      </div>
      <!--/ Dataset. -->

      <!-- Data Acquisition. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 id="Data_Request" class="title is-3">Data Request</h2>
          <div class="content has-text-justified">
            <p>
              <strong>We are preparing the data release, and the request method will be released soon.</strong>
              This dataset is for non-commercial research use only, and requests from commercial companies will not be licensed.
            </p>
          </div>
        </div>
      </div>
      <!--/ Data Acquisition. -->

      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>
          <div class="result_1">
            <video id="result_1" autoplay controls muted loop playsinline height="100%" width="60%">
              <source src="./static/videos/result_1.mp4" type="video/mp4">
            </video>
            <div class="content has-text-justified">
              <span><center>
                <strong>Up: GroundTruth &emsp;&emsp; Down: Ours &emsp;&emsp; Input Emotion: Angry</strong>
              </center></span>
            </div>
          </div>

          <div class="result_2">
            <video id="result_2" autoplay controls muted loop playsinline height="100%" width="60%">
              <source src="./static/videos/result_2.mp4" type="video/mp4">
            </video>
            <div class="content has-text-justified">
              <span><center>
                <strong>Up: GroundTruth &emsp;&emsp; Down: Ours &emsp;&emsp; Input Emotion: Disgusted</strong>
              </center></span>
            </div>
          </div>

          <div class="result_3">
            <video id="result_3" autoplay controls muted loop playsinline height="100%" width="60%">
              <source src="./static/videos/result_3.mp4" type="video/mp4">
            </video>
            <div class="content has-text-justified">
              <span><center>
                <strong>Up: GroundTruth &emsp;&emsp; Down: Ours &emsp;&emsp; Input Emotion: Happy</strong>
              </center></span>
            </div>
          </div>
        </div>
      </div>
      <!--/ Results. -->

      <!-- In-the-wild Audio-driven. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">In-the-wild Audio-driven</h2>
          <div class="novel_results_video">
            <div class="novel_result_1">
              <video id="novel_result_1" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/novel_result_1_cut.mp4" type="video/mp4">
              </video>
            </div>

            <div class="novel_result_2">
              <video id="novel_result_2" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/novel_result_2_cut.mp4" type="video/mp4">
              </video>
            </div>

            <div class="novel_result_3">
              <video id="novel_result_3" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/novel_result_3_cut.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
      <!--/ In-the-wild Audio-driven. -->

      <!-- Free-viewpoint Animation. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Free-viewpoint Animation</h2>
          <div class="free_view_audio">
            <video id="free_view_audio" autoplay controls muted loop playsinline height="100%" width="60%">
              <source src="./static/videos/free_view.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <!--/ Free-viewpoint Animation. -->

      <!-- Video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column column is-max-desktop">
          <h2 id="our_video" class="title is-3">Supplementary Video</h2>
          <!-- 主视频 -->

          <div class="teaser-video-container">
            <video id="teaser" autoplay muted controls playsinline height="100%">
              <source src="./static/videos/paper_video.mp4" type="video/mp4">
            </video>
          </div>
        </div>       
      </div>
      <!-- Video. -->

      <div class="is-size-6 is-centered has-text-centered">
        <a class="back-to-top" href="#" onclick="scrollToTop(); return false;">(back to top)</a>
      </div>    
    <br>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{he2024emotalk3d,
  title={EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head},
  author={He, Qianyun and Ji, Xinya and Gong, Yicheng and Lu, Yuanxun and Diao, Zhengyu and Huang, Linjia and Yao, Yao and Zhu, Siyu and Ma, Zhan and Xu, Songchen and Wu, Xiaofei and Zhang, Zixiao and Cao, Xun and Zhu, Hao},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2024}      
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <!-- 这一部分主要是链接pdf文档和GitHub，如果建好了可以把链接改好上传上去 -->
      <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The website template was borrowed from <a href="https://nerfies.github.io/">Nerfies</a>.
              Thanks to <a href="https://nerfies.github.io/">Nerfies</a>.
            </p>
            <p>
              Like <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>, this website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>